{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient Boosting Regression** is a machine learning technique used to predict continuous target values by combining the predictions of multiple weak learners, typically decision trees, in a sequential manner. It is based on the principle of gradient boosting, where each model in the sequence aims to correct the errors of its predecessor. Here's a detailed breakdown of how it works:\n",
    "\n",
    "### **How Gradient Boosting Regression Works:**\n",
    "\n",
    "1. **Initialization:**\n",
    "   - **Start with a Simple Model:** Begin with an initial model that provides a base prediction. This is often the mean of the target values in the case of regression problems.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - **Calculate Errors:** Determine the residuals, which are the differences between the actual target values and the predictions made by the current model. Residuals represent the errors or the amount that the current model is missing.\n",
    "\n",
    "3. **Train New Model:**\n",
    "   - **Fit to Residuals:** Train a new model (usually a decision tree) to predict the residuals from the previous model. This new model focuses on the errors made by the current model, trying to correct them.\n",
    "\n",
    "4. **Update Predictions:**\n",
    "   - **Add Predictions:** Update the predictions of the ensemble by adding the predictions of the new model, scaled by a learning rate. The learning rate controls how much each new model contributes to the final prediction.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - **Iterate:** Repeat the process of computing residuals, training new models, and updating predictions for a specified number of iterations or until no significant improvements are observed.\n",
    "\n",
    "6. **Final Prediction:**\n",
    "   - **Aggregate Predictions:** The final prediction is the sum of the base prediction and the predictions from all the models trained in the sequence. Each model contributes to the final prediction based on its performance and the learning rate.\n",
    "\n",
    "### **Key Parameters:**\n",
    "\n",
    "- **`n_estimators`**: Number of boosting stages or trees to be used. More trees can lead to better performance but increased computation time.\n",
    "- **`learning_rate`**: Shrinkage factor that scales the contribution of each tree. Lower values can prevent overfitting but may require more trees.\n",
    "- **`max_depth`**: Maximum depth of each decision tree. Controls the complexity of the model.\n",
    "- **`min_samples_split`**: Minimum number of samples required to split an internal node.\n",
    "- **`min_samples_leaf`**: Minimum number of samples required to be at a leaf node.\n",
    "- **`subsample`**: Fraction of samples used to fit each tree. Helps with regularization by introducing randomness.\n",
    "\n",
    "### **Advantages:**\n",
    "\n",
    "- **Accuracy:** Gradient Boosting Regression can achieve high accuracy by combining multiple weak learners.\n",
    "- **Flexibility:** It can handle various types of regression problems and can model complex relationships.\n",
    "- **Feature Importance:** Provides insight into the importance of different features through the learned model.\n",
    "\n",
    "### **Limitations:**\n",
    "\n",
    "- **Computational Cost:** Training can be computationally intensive, especially with a large number of trees.\n",
    "- **Overfitting:** It can overfit the training data if not properly tuned, particularly with too many trees or a high learning rate.\n",
    "- **Complexity:** The resulting model can be complex and harder to interpret compared to simpler models.\n",
    "\n",
    "Gradient Boosting Regression is widely used in practice for its ability to create powerful predictive models, especially in scenarios where accurate predictions are critical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's implement a simple gradient boosting algorithm from scratch using Python and NumPy. We'll use a basic regression problem with a small dataset. The steps will include:\n",
    "\n",
    "1. **Generating a simple dataset.**\n",
    "2. **Implementing the gradient boosting algorithm.**\n",
    "3. **Training the model.**\n",
    "4. **Evaluating the model using metrics like mean squared error (MSE) and R-squared.**\n",
    "\n",
    "### **Step 1: Generate a Simple Dataset**\n",
    "\n",
    "We'll create a synthetic dataset for regression.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y = 2 * X.flatten() + 1 + np.random.normal(0, 1, X.shape[0])\n",
    "\n",
    "# Plot the dataset\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Synthetic Regression Dataset')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Step 2: Implement the Gradient Boosting Algorithm**\n",
    "\n",
    "We'll implement a simple version of gradient boosting using decision stumps (shallow trees) as weak learners.\n",
    "\n",
    "```python\n",
    "class DecisionStump:\n",
    "    def __init__(self):\n",
    "        self.feature_index = None\n",
    "        self.threshold = None\n",
    "        self.value_left = None\n",
    "        self.value_right = None\n",
    "\n",
    "    def fit(self, X, y, sample_weights):\n",
    "        # Find the best split\n",
    "        best_loss = float('inf')\n",
    "        for feature_index in range(X.shape[1]):\n",
    "            thresholds = np.unique(X[:, feature_index])\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature_index] < threshold\n",
    "                right_indices = X[:, feature_index] >= threshold\n",
    "\n",
    "                if len(y[left_indices]) == 0 or len(y[right_indices]) == 0:\n",
    "                    continue\n",
    "\n",
    "                left_value = np.average(y[left_indices], weights=sample_weights[left_indices])\n",
    "                right_value = np.average(y[right_indices], weights=sample_weights[right_indices])\n",
    "                \n",
    "                predictions = np.where(left_indices, left_value, right_value)\n",
    "                loss = np.sum(sample_weights * (y - predictions) ** 2)\n",
    "\n",
    "                if loss < best_loss:\n",
    "                    best_loss = loss\n",
    "                    self.feature_index = feature_index\n",
    "                    self.threshold = threshold\n",
    "                    self.value_left = left_value\n",
    "                    self.value_right = right_value\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.where(X[:, self.feature_index] < self.threshold, self.value_left, self.value_right)\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        # Initialize predictions with zeros\n",
    "        predictions = np.zeros(y.shape)\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Compute residuals\n",
    "            residuals = y - predictions\n",
    "            # Train a new decision stump on residuals\n",
    "            model = DecisionStump()\n",
    "            model.fit(X, residuals, sample_weights=np.ones_like(y))\n",
    "            # Update predictions\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "### **Step 3: Train the Model**\n",
    "\n",
    "We'll train the gradient boosting model on our synthetic dataset.\n",
    "\n",
    "```python\n",
    "# Create and train the gradient boosting model\n",
    "model = GradientBoostingRegressor(n_estimators=50, learning_rate=0.1)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predict on the training data\n",
    "y_pred = model.predict(X)\n",
    "```\n",
    "\n",
    "### **Step 4: Evaluate the Model**\n",
    "\n",
    "We'll evaluate the model's performance using MSE and R-squared metrics.\n",
    "\n",
    "```python\n",
    "# Calculate performance metrics\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_pred, color='red', label='Gradient Boosting Predictions')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "1. **Data Generation:** We created a synthetic dataset for regression.\n",
    "2. **Gradient Boosting Algorithm:** We implemented a simple gradient boosting algorithm using decision stumps as weak learners.\n",
    "3. **Model Training:** We trained the model and made predictions.\n",
    "4. **Evaluation:** We evaluated the model using MSE and R-squared metrics and visualized the results.\n",
    "\n",
    "Feel free to run this code and adjust the parameters or dataset to further explore the behavior of gradient boosting regression!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To experiment with different hyperparameters and optimize the performance of our gradient boosting model, we can use techniques like grid search or random search. In this case, we will implement grid search to find the best combination of hyperparameters. We'll focus on tuning the learning rate, number of trees (estimators), and tree depth.\n",
    "\n",
    "### **Step 1: Modify the Gradient Boosting Implementation**\n",
    "\n",
    "First, we need to allow for different tree depths in our `DecisionStump` implementation. Instead of using a single depth, we'll use decision trees of variable depths.\n",
    "\n",
    "Here’s a slightly modified implementation using `DecisionTreeRegressor` from `sklearn` to handle variable tree depths:\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=10, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "        predictions = np.zeros(y.shape)\n",
    "        for _ in range(self.n_estimators):\n",
    "            residuals = y - predictions\n",
    "            model = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            model.fit(X, residuals)\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "            self.models.append(model)\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros(X.shape[0])\n",
    "        for model in self.models:\n",
    "            predictions += self.learning_rate * model.predict(X)\n",
    "        return predictions\n",
    "```\n",
    "\n",
    "### **Step 2: Implement Grid Search for Hyperparameter Tuning**\n",
    "\n",
    "We will use grid search to explore combinations of learning rates, number of trees, and tree depths.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create a scoring function for mean squared error\n",
    "scoring = make_scorer(mean_squared_error, greater_is_better=False)\n",
    "\n",
    "# Grid search using cross-validation\n",
    "best_model = None\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for n_estimators in param_grid['n_estimators']:\n",
    "    for learning_rate in param_grid['learning_rate']:\n",
    "        for max_depth in param_grid['max_depth']:\n",
    "            model = GradientBoostingRegressor(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth)\n",
    "            model.fit(X, y)\n",
    "            y_pred = model.predict(X)\n",
    "            score = mean_squared_error(y, y_pred)\n",
    "            \n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_params = {'n_estimators': n_estimators, 'learning_rate': learning_rate, 'max_depth': max_depth}\n",
    "                best_model = model\n",
    "\n",
    "print(\"Best Parameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Mean Squared Error: {best_score:.4f}\")\n",
    "\n",
    "# Plot the results with the best model\n",
    "y_pred_best = best_model.predict(X)\n",
    "\n",
    "plt.scatter(X, y, color='blue', label='Data')\n",
    "plt.plot(X, y_pred_best, color='red', label='Best Gradient Boosting Predictions')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Best Gradient Boosting Regression')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **Step 3: Analysis**\n",
    "\n",
    "- **Grid Search:** The code iterates over all combinations of the specified hyperparameters to find the best combination based on the mean squared error.\n",
    "- **Results:** The grid search will print the best parameters and the corresponding mean squared error. It also plots the predictions of the best model.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "1. **Modified Model:** Updated the gradient boosting model to support different tree depths.\n",
    "2. **Grid Search:** Used grid search to find the optimal combination of learning rate, number of trees, and tree depth.\n",
    "3. **Evaluation:** Evaluated the performance of the model and visualized the results.\n",
    "\n",
    "Feel free to run the code and adjust the parameter grid to further explore and optimize the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of Gradient Boosting, a **weak learner** (or weak model) is a simple, basic model that performs only slightly better than random guessing. The idea is to use these weak learners in combination to build a strong learner that can make accurate predictions. Here’s a detailed explanation:\n",
    "\n",
    "### **Characteristics of Weak Learners:**\n",
    "\n",
    "1. **Simple Structure:** Weak learners are typically simple models that have limited capacity to capture complex patterns in the data. For instance, a decision tree with just one level (a decision stump) is often used as a weak learner.\n",
    "\n",
    "2. **Low Bias:** While weak learners might have high variance (they can overfit the data if not constrained), they generally have low bias, meaning they can approximate the relationship between features and the target variable reasonably well.\n",
    "\n",
    "3. **Incremental Improvement:** Each weak learner in a boosting algorithm is designed to correct the errors of the ensemble of previously trained weak learners. The process is iterative, where each new model improves upon the previous ones by focusing on the residuals or errors.\n",
    "\n",
    "### **Role of Weak Learners in Gradient Boosting:**\n",
    "\n",
    "1. **Sequential Learning:** In Gradient Boosting, weak learners are trained sequentially. Each new weak learner is trained to predict the residuals (errors) of the combined predictions from all previous models. This sequential approach allows each weak learner to address the shortcomings of the ensemble so far.\n",
    "\n",
    "2. **Combination of Models:** Although individual weak learners are simple, their combined effect through boosting can lead to a powerful and accurate model. By aggregating the predictions from multiple weak learners, Gradient Boosting can capture complex patterns and improve performance.\n",
    "\n",
    "3. **Error Correction:** Each weak learner contributes to reducing the errors made by the previous ensemble. By focusing on the residuals, the new models correct mistakes, leading to improved predictions over time.\n",
    "\n",
    "### **Examples of Weak Learners:**\n",
    "\n",
    "- **Decision Trees:** Often, very shallow decision trees (also called decision stumps) are used as weak learners in boosting algorithms. They are simple and have limited depth, making them suitable for capturing basic patterns.\n",
    "\n",
    "- **Linear Models:** In some boosting algorithms, linear models with a small number of features might be used as weak learners.\n",
    "\n",
    "- **Other Models:** Any simple model that can make predictions with minimal complexity can serve as a weak learner. For instance, linear regression with a single feature or a very basic neural network.\n",
    "\n",
    "### **Why Use Weak Learners?**\n",
    "\n",
    "- **Simplicity:** Weak learners are computationally inexpensive and easy to implement.\n",
    "- **Flexibility:** They can be combined in a flexible manner to handle different types of data and complex relationships.\n",
    "- **Overfitting Control:** Using weak learners helps in controlling overfitting since they are constrained in complexity, and boosting techniques like regularization further help in this regard.\n",
    "\n",
    "By combining multiple weak learners through boosting, you leverage their collective strength to create a strong learner that can make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intuition behind the Gradient Boosting algorithm revolves around improving predictive accuracy by incrementally correcting errors made by previous models. Here's a step-by-step explanation of the core concepts:\n",
    "\n",
    "### **1. Ensemble Learning:**\n",
    "   - **Combine Models:** Gradient Boosting is an ensemble learning technique that combines multiple weak learners (simple models) to form a stronger model. The idea is that while each individual weak learner might perform poorly, their combined output can achieve high accuracy.\n",
    "\n",
    "### **2. Sequential Correction:**\n",
    "   - **Iterative Training:** Gradient Boosting trains models sequentially. Each new model is trained to correct the errors of the combined previous models. This iterative process helps in gradually improving the overall prediction accuracy.\n",
    "\n",
    "### **3. Residuals and Errors:**\n",
    "   - **Focus on Mistakes:** After training an initial model, compute the residuals (errors) which are the differences between the actual target values and the predictions made by the current model. Subsequent models are trained specifically to predict these residuals, thereby focusing on the mistakes made by previous models.\n",
    "\n",
    "### **4. Gradient Descent Analogy:**\n",
    "   - **Minimize Loss:** The algorithm is inspired by gradient descent. Instead of updating parameters of a single model, it updates the ensemble of models to minimize the loss function (e.g., mean squared error). Each new model is fit to the negative gradient of the loss function with respect to the current predictions.\n",
    "\n",
    "### **5. Learning Rate:**\n",
    "   - **Shrinkage:** The learning rate (or step size) controls how much each new model contributes to the final prediction. A smaller learning rate requires more iterations to converge but can lead to better generalization and prevent overfitting.\n",
    "\n",
    "### **6. Boosting Process:**\n",
    "   - **Start Simple:** Begin with a simple model that provides an initial prediction.\n",
    "   - **Add Models:** Train additional models to predict the residuals (errors) from the previous models.\n",
    "   - **Combine Models:** Update the overall predictions by adding the predictions of the new model, scaled by the learning rate.\n",
    "   - **Repeat:** Continue this process for a specified number of iterations or until performance improvements are minimal.\n",
    "\n",
    "### **Visualization of Gradient Boosting:**\n",
    "\n",
    "1. **Initial Model:** Start with an initial model that makes a simple prediction.\n",
    "   - **Example:** A model predicting the mean of the target values.\n",
    "\n",
    "2. **Compute Residuals:** Calculate the difference between the actual values and the predictions from the initial model.\n",
    "\n",
    "3. **Fit New Model:** Train a new model to predict these residuals.\n",
    "   - **Example:** A decision tree trained to fit the residuals.\n",
    "\n",
    "4. **Update Predictions:** Combine the predictions from the new model with the previous predictions, adjusting by the learning rate.\n",
    "\n",
    "5. **Iterate:** Repeat the process with updated residuals, training new models and adjusting predictions until the ensemble converges.\n",
    "\n",
    "### **Key Benefits:**\n",
    "\n",
    "- **Improved Accuracy:** By focusing on the errors of previous models, Gradient Boosting can improve accuracy significantly.\n",
    "- **Flexibility:** It can handle various types of predictive problems and adapt to complex data patterns.\n",
    "- **Robustness:** The ensemble approach helps in reducing overfitting and making the model more robust.\n",
    "\n",
    "### **Summary:**\n",
    "\n",
    "Gradient Boosting builds a strong predictive model by combining multiple weak learners in a sequential manner, focusing on correcting errors made by previous models, and iteratively improving predictions. The learning rate and iterative updates play crucial roles in refining the model and achieving high accuracy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
