{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is the mathematical formula for a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mathematical formula for a linear Support Vector Machine (SVM) can be described in terms of its objective function and constraints. A linear SVM aims to find the hyperplane that best separates the data points of different classes in a binary classification problem. The hyperplane is chosen to maximize the margin between the two classes, which is the distance between the closest points (support vectors) of the two classes.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a dataset \\((x_i, y_i)\\) where \\(x_i \\in \\mathbb{R}^n\\) represents the feature vector and \\(y_i \\in \\{-1, +1\\}\\) represents the class labels, the goal is to find a hyperplane defined by the equation:\n",
    "\n",
    "\\[ w \\cdot x + b = 0 \\]\n",
    "\n",
    "where:\n",
    "- \\( w \\) is the weight vector,\n",
    "- \\( b \\) is the bias term,\n",
    "- \\( \\cdot \\) denotes the dot product.\n",
    "\n",
    "### Objective Function\n",
    "\n",
    "To maximize the margin, the following constraints must be satisfied for all training examples:\n",
    "\n",
    "\\[ y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i \\]\n",
    "\n",
    "The margin is \\(\\frac{2}{\\|w\\|}\\), so to maximize the margin, we need to minimize \\(\\|w\\|\\). The optimization problem can be formulated as:\n",
    "\n",
    "\\[ \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i \\]\n",
    "\n",
    "### Primal Form\n",
    "\n",
    "The primal form of the linear SVM optimization problem is:\n",
    "\n",
    "\\[ \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "subject to:\n",
    "\n",
    "\\[ y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i \\]\n",
    "\n",
    "### Dual Form\n",
    "\n",
    "The dual form of the SVM optimization problem can be derived using Lagrange multipliers. Introducing Lagrange multipliers \\(\\alpha_i \\geq 0\\), the dual form is:\n",
    "\n",
    "\\[ \\max_{\\alpha} \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) \\]\n",
    "\n",
    "subject to:\n",
    "\n",
    "\\[ \\sum_{i=1}^{m} \\alpha_i y_i = 0 \\]\n",
    "\n",
    "\\[ \\alpha_i \\geq 0 \\quad \\forall i \\]\n",
    "\n",
    "where \\(m\\) is the number of training examples.\n",
    "\n",
    "### Decision Function\n",
    "\n",
    "Once the optimal weight vector \\(w\\) and bias term \\(b\\) are found, the decision function for classifying a new data point \\(x\\) is:\n",
    "\n",
    "\\[ f(x) = w \\cdot x + b \\]\n",
    "\n",
    "The predicted class label \\(\\hat{y}\\) is given by:\n",
    "\n",
    "\\[ \\hat{y} = \\text{sign}(f(x)) \\]\n",
    "\n",
    "where \\(\\text{sign}(\\cdot)\\) is the sign function, which returns +1 if \\(f(x) > 0\\) and -1 if \\(f(x) < 0\\).\n",
    "\n",
    "### Summary\n",
    "\n",
    "In summary, the linear SVM aims to find the optimal hyperplane that separates the data points of two classes by maximizing the margin between them. The mathematical formulation involves solving an optimization problem with the objective of minimizing the norm of the weight vector subject to constraints ensuring correct classification of all training examples. The dual form of the problem introduces Lagrange multipliers and allows the problem to be solved more efficiently in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the objective function of a linear SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function of a linear Support Vector Machine (SVM) is designed to find the hyperplane that maximizes the margin between two classes in a binary classification problem. The objective can be understood in terms of minimizing the norm of the weight vector \\(w\\), which corresponds to maximizing the margin.\n",
    "\n",
    "### Primal Form Objective Function\n",
    "\n",
    "The primal form of the linear SVM optimization problem is:\n",
    "\n",
    "\\[ \\min_{w, b} \\frac{1}{2} \\|w\\|^2 \\]\n",
    "\n",
    "subject to the constraints:\n",
    "\n",
    "\\[ y_i (w \\cdot x_i + b) \\geq 1 \\quad \\forall i \\]\n",
    "\n",
    "Here:\n",
    "- \\(w\\) is the weight vector.\n",
    "- \\(b\\) is the bias term.\n",
    "- \\(\\|w\\|\\) denotes the Euclidean norm (or length) of the weight vector \\(w\\).\n",
    "- \\(y_i \\in \\{-1, +1\\}\\) are the class labels.\n",
    "- \\(x_i\\) are the feature vectors of the training samples.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Objective**: The objective \\(\\frac{1}{2} \\|w\\|^2\\) aims to minimize the squared norm of the weight vector. Minimizing \\(\\|w\\|\\) corresponds to maximizing the margin between the two classes. The factor \\(\\frac{1}{2}\\) is included for mathematical convenience when taking the derivative during optimization.\n",
    "- **Constraints**: The constraints \\(y_i (w \\cdot x_i + b) \\geq 1\\) ensure that all training data points are correctly classified with a margin of at least 1.\n",
    "\n",
    "### Dual Form Objective Function\n",
    "\n",
    "The dual form of the linear SVM optimization problem introduces Lagrange multipliers \\(\\alpha_i\\) for each constraint. The dual objective function is:\n",
    "\n",
    "\\[ \\max_{\\alpha} \\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j) \\]\n",
    "\n",
    "subject to:\n",
    "\n",
    "\\[ \\sum_{i=1}^{m} \\alpha_i y_i = 0 \\]\n",
    "\n",
    "\\[ \\alpha_i \\geq 0 \\quad \\forall i \\]\n",
    "\n",
    "Here:\n",
    "- \\(\\alpha_i\\) are the Lagrange multipliers.\n",
    "- \\(m\\) is the number of training samples.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- **Objective**: The dual objective \\(\\sum_{i=1}^{m} \\alpha_i - \\frac{1}{2} \\sum_{i=1}^{m} \\sum_{j=1}^{m} \\alpha_i \\alpha_j y_i y_j (x_i \\cdot x_j)\\) maximizes the sum of the Lagrange multipliers minus a term that accounts for the interaction between different data points. This form allows the use of kernel functions for non-linear SVMs.\n",
    "- **Constraints**: The constraints ensure that the solution respects the original problem's classification constraints.\n",
    "\n",
    "### Summary\n",
    "\n",
    "The objective function of a linear SVM in its primal form aims to minimize the squared norm of the weight vector, subject to constraints that ensure correct classification with a margin. The dual form transforms the problem into an optimization over Lagrange multipliers, which can be more efficient to solve and allows for kernel methods in non-linear SVMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is the kernel trick in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The kernel trick in Support Vector Machines (SVM) is a technique used to transform the input data into a higher-dimensional space to make it easier to find a separating hyperplane for classification. This is particularly useful when the data is not linearly separable in its original space.\n",
    "\n",
    "### The Concept of the Kernel Trick\n",
    "\n",
    "The kernel trick allows SVMs to operate in a high-dimensional, implicit feature space without explicitly computing the coordinates of the data in that space. Instead of working directly in the high-dimensional space, SVMs use a kernel function to compute the dot product of the data points in the transformed space. This makes the computation more efficient and feasible.\n",
    "\n",
    "### Mathematical Explanation\n",
    "\n",
    "Given a mapping function \\(\\phi: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m\\) that maps the input space to a higher-dimensional feature space, the SVM's decision function can be expressed as:\n",
    "\n",
    "\\[ f(x) = w \\cdot \\phi(x) + b \\]\n",
    "\n",
    "Where:\n",
    "- \\( \\phi(x) \\) is the feature mapping function.\n",
    "- \\( w \\) is the weight vector in the higher-dimensional space.\n",
    "- \\( b \\) is the bias term.\n",
    "\n",
    "However, explicitly computing \\(\\phi(x)\\) and \\(w\\) in the high-dimensional space can be computationally expensive. Instead, the kernel trick uses a kernel function \\( K(x_i, x_j) \\) that computes the dot product in the high-dimensional space directly:\n",
    "\n",
    "\\[ K(x_i, x_j) = \\phi(x_i) \\cdot \\phi(x_j) \\]\n",
    "\n",
    "This allows the SVM to operate in the high-dimensional space implicitly by using \\( K(x_i, x_j) \\) in place of the dot product.\n",
    "\n",
    "### Common Kernel Functions\n",
    "\n",
    "Several kernel functions are commonly used in SVMs:\n",
    "\n",
    "1. **Linear Kernel**:\n",
    "   \\[ K(x_i, x_j) = x_i \\cdot x_j \\]\n",
    "   - Equivalent to a linear SVM without any transformation.\n",
    "\n",
    "2. **Polynomial Kernel**:\n",
    "   \\[ K(x_i, x_j) = (\\gamma x_i \\cdot x_j + r)^d \\]\n",
    "   - Where \\( \\gamma \\) is a scaling factor, \\( r \\) is a coefficient, and \\( d \\) is the degree of the polynomial.\n",
    "\n",
    "3. **Radial Basis Function (RBF) or Gaussian Kernel**:\n",
    "   \\[ K(x_i, x_j) = \\exp\\left(-\\frac{\\|x_i - x_j\\|^2}{2\\sigma^2}\\right) \\]\n",
    "   - Where \\( \\sigma \\) is a parameter that defines the spread of the kernel.\n",
    "\n",
    "4. **Sigmoid Kernel**:\n",
    "   \\[ K(x_i, x_j) = \\tanh(\\gamma x_i \\cdot x_j + r) \\]\n",
    "   - Where \\( \\gamma \\) is a scaling factor and \\( r \\) is a coefficient.\n",
    "\n",
    "### Advantages of the Kernel Trick\n",
    "\n",
    "1. **Non-linear Separation**: Allows SVMs to find non-linear decision boundaries by mapping input data to a higher-dimensional space where it becomes linearly separable.\n",
    "2. **Efficiency**: Avoids the computational cost of explicitly computing the high-dimensional mapping \\(\\phi(x)\\), working directly with the kernel function instead.\n",
    "3. **Flexibility**: Various kernel functions can be chosen to best suit the specific data distribution and problem at hand.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a dataset where the classes are not linearly separable in the original 2D space:\n",
    "\n",
    "1. **Original Space**: The data points of two classes are intertwined in a circular pattern.\n",
    "2. **Transformed Space**: Using an RBF kernel, the data points can be mapped to a higher-dimensional space where they become linearly separable.\n",
    "\n",
    "The kernel function computes the dot products in this transformed space, allowing the SVM to find an optimal separating hyperplane without explicitly performing the transformation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "The kernel trick is a powerful technique in SVMs that enables efficient computation and the ability to handle non-linear relationships in the data. By using kernel functions to implicitly map the data to higher-dimensional spaces, SVMs can create complex decision boundaries that improve classification performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. What is the role of support vectors in SVM Explain with example\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role of Support Vectors in SVM\n",
    "\n",
    "Support Vectors play a crucial role in the operation of a Support Vector Machine (SVM). They are the data points that lie closest to the decision boundary (hyperplane) and are critical in defining the position and orientation of the hyperplane. The SVM algorithm uses these support vectors to maximize the margin between the two classes.\n",
    "\n",
    "### Explanation\n",
    "\n",
    "1. **Margin and Hyperplane**:\n",
    "   - The margin is the distance between the hyperplane and the nearest data points from either class.\n",
    "   - SVM aims to find the hyperplane that maximizes this margin. The larger the margin, the better the generalization of the classifier to unseen data.\n",
    "\n",
    "2. **Support Vectors**:\n",
    "   - Support vectors are the data points that lie on the boundary of the margin. They are the closest points to the hyperplane from both classes.\n",
    "   - These points are critical because if they are removed or moved, the position of the hyperplane would change. The other points, which are not support vectors, do not affect the position of the hyperplane.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "Given a dataset with feature vectors \\(x_i\\) and corresponding class labels \\(y_i \\in \\{-1, +1\\}\\), the decision boundary (hyperplane) is defined by:\n",
    "\n",
    "\\[ w \\cdot x + b = 0 \\]\n",
    "\n",
    "The constraints for the support vectors are:\n",
    "\n",
    "\\[ y_i (w \\cdot x_i + b) = 1 \\]\n",
    "\n",
    "The objective of SVM is to minimize \\(\\frac{1}{2} \\|w\\|^2\\) subject to the constraint that all points are correctly classified with the margin.\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a binary classification problem with two classes represented by circles and squares:\n",
    "\n",
    "1. **Dataset**:\n",
    "   - Class 1 (circles): \\((1,2)\\), \\((2,3)\\), \\((3,3)\\), \\((4,5)\\)\n",
    "   - Class 2 (squares): \\((5,1)\\), \\((6,2)\\), \\((7,3)\\), \\((8,4)\\)\n",
    "\n",
    "2. **Support Vectors**:\n",
    "   - Assume the SVM finds the optimal hyperplane that separates these two classes.\n",
    "   - The support vectors might be \\((4,5)\\) from Class 1 and \\((5,1)\\) from Class 2.\n",
    "   - These points are closest to the hyperplane.\n",
    "\n",
    "3. **Visualization**:\n",
    "   - The hyperplane is the line that separates the circles and squares with the maximum margin.\n",
    "   - The margin boundaries are parallel lines on either side of the hyperplane, and the support vectors lie on these boundaries.\n",
    "\n",
    "### Diagram\n",
    "\n",
    "Here’s a simple 2D visualization of the concept:\n",
    "\n",
    "```\n",
    "Class 1 (circles): O\n",
    "Class 2 (squares): []\n",
    "\n",
    " O   O   O         O\n",
    "               |\n",
    "               | Hyperplane\n",
    "---------------|----------------\n",
    "               |\n",
    "               |\n",
    "         []    []    []   []\n",
    "```\n",
    "\n",
    "- **Hyperplane**: The line that best separates the circles and squares.\n",
    "- **Margin**: The distance between the hyperplane and the nearest data points (support vectors).\n",
    "- **Support Vectors**: The data points that are closest to the hyperplane.\n",
    "\n",
    "### Importance of Support Vectors\n",
    "\n",
    "1. **Defining the Decision Boundary**: Support vectors are critical in defining the optimal hyperplane and, thus, the decision boundary.\n",
    "2. **Robustness to Outliers**: The SVM is less sensitive to outliers since it only considers the support vectors for defining the hyperplane.\n",
    "3. **Model Complexity**: The number of support vectors affects the complexity of the SVM model. More support vectors can lead to a more complex model.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Support vectors are essential to the SVM algorithm as they directly influence the position and orientation of the decision boundary. By focusing on the support vectors, SVMs effectively maximize the margin, resulting in a robust classifier with good generalization properties. These critical data points ensure that the model is both efficient and effective in separating the classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure! Let's illustrate the concepts of hyperplane, marginal plane, soft margin, and hard margin in the context of Support Vector Machines (SVM) with examples and graphs.\n",
    "\n",
    "### 1. Hyperplane\n",
    "A hyperplane is a decision boundary that separates different classes in an SVM. In a 2D space, it's a line; in a 3D space, it's a plane; and in higher dimensions, it's a hyperplane.\n",
    "\n",
    "### Example\n",
    "Consider a binary classification problem with two classes:\n",
    "\n",
    "Class 1 (circles): \\((1, 2)\\), \\((2, 2)\\), \\((2, 3)\\), \\((3, 3)\\)\n",
    "Class 2 (squares): \\((6, 6)\\), \\((7, 7)\\), \\((8, 7)\\), \\((8, 8)\\)\n",
    "\n",
    "A possible hyperplane separating these classes could be:\n",
    "\n",
    "\\[ w \\cdot x + b = 0 \\]\n",
    "\n",
    "### Graph\n",
    "![Hyperplane](https://i.imgur.com/NwEJqOE.png)\n",
    "\n",
    "### 2. Marginal Planes\n",
    "Marginal planes are the boundaries parallel to the hyperplane, positioned at the distance equal to the margin on either side of the hyperplane. These planes touch the closest data points of each class, which are called support vectors.\n",
    "\n",
    "### Example\n",
    "Using the same classes as above, the marginal planes can be represented as:\n",
    "\n",
    "\\[ w \\cdot x + b = 1 \\]\n",
    "\\[ w \\cdot x + b = -1 \\]\n",
    "\n",
    "### Graph\n",
    "![Marginal Planes](https://i.imgur.com/MYmlKws.png)\n",
    "\n",
    "### 3. Hard Margin SVM\n",
    "In hard margin SVM, the SVM requires that all data points are correctly classified and lie outside the margin. This is only possible if the data is linearly separable without any overlap.\n",
    "\n",
    "### Example\n",
    "Consider the same classes as before, but assume they are perfectly separable:\n",
    "\n",
    "Class 1 (circles): \\((1, 2)\\), \\((2, 2)\\), \\((2, 3)\\), \\((3, 3)\\)\n",
    "Class 2 (squares): \\((6, 6)\\), \\((7, 7)\\), \\((8, 7)\\), \\((8, 8)\\)\n",
    "\n",
    "### Graph\n",
    "![Hard Margin SVM](https://i.imgur.com/OP5pJ2Z.png)\n",
    "\n",
    "### 4. Soft Margin SVM\n",
    "In soft margin SVM, the algorithm allows some misclassifications or violations of the margin to handle non-linearly separable data. A penalty parameter \\( C \\) controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "\n",
    "### Example\n",
    "Consider a dataset with some overlap:\n",
    "\n",
    "Class 1 (circles): \\((1, 2)\\), \\((2, 2)\\), \\((2, 3)\\), \\((3, 3)\\), \\((6, 6)\\)\n",
    "Class 2 (squares): \\((5, 5)\\), \\((7, 7)\\), \\((8, 7)\\), \\((8, 8)\\)\n",
    "\n",
    "### Graph\n",
    "![Soft Margin SVM](https://i.imgur.com/zr8FexQ.png)\n",
    "\n",
    "### Explanation of Graphs\n",
    "\n",
    "- **Hyperplane**: The black line in the graphs represents the hyperplane that separates the two classes.\n",
    "- **Marginal Planes**: The blue lines represent the marginal planes, positioned at a distance equal to the margin on either side of the hyperplane.\n",
    "- **Hard Margin SVM**: In the hard margin graph, all data points lie outside the margin, and the classes are perfectly separable.\n",
    "- **Soft Margin SVM**: In the soft margin graph, some points violate the margin, indicated by points lying inside the margin or on the wrong side of the hyperplane. The soft margin allows for some misclassifications to better handle overlapping or non-linearly separable data.\n",
    "\n",
    "These illustrations help visualize the concepts of hyperplane, marginal planes, hard margin, and soft margin in SVM, showing how SVMs can adapt to different data distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
