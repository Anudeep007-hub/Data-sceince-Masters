{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Ans: Min max scaling is used to fit the given data points into in range \n",
    "between 0 and 1 using formula, \n",
    "                            X_scaled = (x(i) - x_min)/(x_max - x_min) \n",
    "Then the transformed data is fitted into the required macchine leanring model for\n",
    "required prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustration by code \n",
    "import numpy as np\n",
    "import seaborn as sns  \n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "\n",
    "data = [[1,23],[89,43],[22,45],[11,23]]\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(data) \n",
    "\n",
    "# The output is the normalized numerical values of the given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: UNIT vector is a one of the method of Feature Scaling it converts the every data point to it's normal form by dividing with it's magnitude.\n",
    "But in MIn-Max scaling the data points are converted in the range[0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# Example dataset\n",
    "data = np.array([[160, 50],\n",
    "                 [170, 60],\n",
    "                 [180, 70]])\n",
    "\n",
    "# Initialize Normalizer with l2 norm\n",
    "normalizer = Normalizer(norm='l2')\n",
    "\n",
    "# Fit the Normalizer to the data and transform it\n",
    "unit_scaled_data = normalizer.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Unit Vector Scaled Data:\")\n",
    "print(unit_scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Principal Component Analysis (PCA) is a dimensionality reduction technique used to identify patterns in data and to represent it in a more compact form. It achieves this by transforming the original features into a new set of orthogonal (uncorrelated) features called principal components. These principal components are ordered by the amount of variance they explain in the data, with the first principal component explaining the most variance, followed by the second, and so on.\n",
    "\n",
    "PCA works by finding the directions (principal components) in which the data varies the most. It then projects the data onto these directions, resulting in a lower-dimensional representation while preserving the maximum amount of variance in the data.\n",
    "\n",
    "Here's how PCA is typically applied:\n",
    "\n",
    "1. **Standardize the data**: PCA is sensitive to the scale of the features, so it's important to standardize the data (subtract mean and divide by standard deviation) before applying PCA.\n",
    "\n",
    "2. **Compute the covariance matrix**: PCA calculates the covariance matrix of the standardized data, which represents the relationships between all pairs of features.\n",
    "\n",
    "3. **Compute the eigenvectors and eigenvalues of the covariance matrix**: PCA decomposes the covariance matrix into its eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance (principal components), while eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "\n",
    "4. **Select the principal components**: PCA selects the top \\( k \\) eigenvectors (principal components) corresponding to the \\( k \\) largest eigenvalues to retain the most important information in the data.\n",
    "\n",
    "5. **Transform the data**: Finally, PCA projects the original data onto the selected principal components to obtain the lower-dimensional representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example of how PCA can be applied using Python's scikit-learn library:\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target variable\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Initialize PCA with 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the standardized data\n",
    "X_pca = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "# Print the transformed data\n",
    "print(\"Transformed Data:\")\n",
    "print(X_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: PCA and feature extraction are closely related concepts, as PCA can be used as a feature extraction technique. Feature extraction refers to the process of deriving new features from the original set of features in a dataset. It aims to reduce the dimensionality of the data while preserving the most important information.\n",
    "\n",
    "PCA achieves feature extraction by transforming the original features into a new set of orthogonal features called principal components. These principal components are linear combinations of the original features and capture the directions of maximum variance in the data. By selecting a subset of principal components that explain the most variance, PCA effectively extracts the most informative features from the original dataset.\n",
    "\n",
    "Here's how PCA can be used for feature extraction:\n",
    "\n",
    "Standardize the data: As with PCA for dimensionality reduction, it's important to standardize the data (subtract mean and divide by standard deviation) before applying PCA for feature extraction.\n",
    "1. Compute the covariance matrix: PCA calculates the covariance matrix of the standardized data, representing the relationships between all pairs of features.\n",
    "2. Compute the eigenvectors and eigenvalues: PCA decomposes the covariance matrix into its eigenvectors and eigenvalues. Eigenvectors represent the directions of maximum variance (principal components), while eigenvalues represent the amount of variance explained by each eigenvector.\n",
    "3. Select the principal components: PCA selects the top k eigenvectors (principal components) corresponding to the k largest eigenvalues to retain the most important information in the data.\n",
    "4. Transform the data: Finally, PCA projects the original data onto the selected principal components to obtain the lower-dimensional representation, effectively extracting new features from the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data  # Features\n",
    "y = iris.target  # Target variable\n",
    "\n",
    "# Standardize the features\n",
    "X_standardized = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "\n",
    "# Initialize PCA with 2 principal components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit and transform the standardized data\n",
    "X_extracted_features = pca.fit_transform(X_standardized)\n",
    "\n",
    "# Print the transformed data\n",
    "print(\"Extracted Features:\")\n",
    "print(X_extracted_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: To use Min-Max scaling to preprocess the data for building a recommendation system for a food delivery service, you would follow these steps:\n",
    "\n",
    "1. **Understand the dataset**: First, carefully examine the dataset to identify the features that need to be scaled. In this case, the features could include price, rating, and delivery time.\n",
    "\n",
    "2. **Perform Min-Max scaling**: Apply Min-Max scaling to each feature individually. This process will rescale each feature to a range between 0 and 1, ensuring that they all have a similar scale.\n",
    "\n",
    "3. **Normalization formula**: Utilize the Min-Max scaling formula for each feature:\n",
    "   \\[ X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}} \\]\n",
    "   Where:\n",
    "   - \\( X \\) is the original value of the feature.\n",
    "   - \\( X_{\\text{min}} \\) is the minimum value of the feature in the dataset.\n",
    "   - \\( X_{\\text{max}} \\) is the maximum value of the feature in the dataset.\n",
    "   - \\( X_{\\text{scaled}} \\) is the scaled value of the feature.\n",
    "\n",
    "4. **Implement scaling**: Use a library like scikit-learn in Python to implement Min-Max scaling. Here's a basic example:\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming 'data' is your dataset containing features like price, rating, and delivery time\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Apply Min-Max scaling to each feature individually\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "```\n",
    "\n",
    "5. **Verify the scaled data**: Check the scaled data to ensure that each feature now falls within the desired range of 0 to 1.\n",
    "\n",
    "6. **Use scaled data for recommendation system**: Utilize the scaled data in building your recommendation system. The scaled features will now have a similar scale, preventing any one feature from dominating the recommendation process due to its larger magnitude.\n",
    "\n",
    "By applying Min-Max scaling, you ensure that features such as price, rating, and delivery time are on the same scale, which can lead to more accurate recommendations in your food delivery service recommendation system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q6.You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using PCA to reduce the dimensionality of the dataset for predicting stock prices involves the following steps:\n",
    "\n",
    "1. **Understand the dataset**: Thoroughly examine the dataset containing features such as company financial data (e.g., revenue, earnings, debt-to-equity ratio) and market trends (e.g., stock market indices, interest rates, economic indicators).\n",
    "\n",
    "2. **Standardize the data**: Since PCA is sensitive to the scale of the features, it's important to standardize the data before applying PCA. Standardization involves subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "3. **Apply PCA**: Once the data is standardized, apply PCA to the dataset to identify the principal components. PCA will find the directions in which the data varies the most and represent the original features in terms of these principal components.\n",
    "\n",
    "4. **Determine the number of components**: Decide on the number of principal components to retain. This decision can be based on the cumulative explained variance ratio. It's common to retain enough components to explain a significant portion (e.g., 80-90%) of the total variance in the data.\n",
    "\n",
    "5. **Transform the data**: Transform the standardized data using the selected principal components. This will result in a lower-dimensional representation of the dataset, where each data point is represented by a reduced set of features (the principal components).\n",
    "\n",
    "6. **Model training**: Use the reduced-dimensional dataset for training your stock price prediction model. Since the dataset now contains fewer features, training the model may be computationally faster and less prone to overfitting.\n",
    "\n",
    "Here's a high-level overview of how you could implement PCA for dimensionality reduction in Python using scikit-learn:\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Assuming 'data' is your dataset containing features\n",
    "# Step 2: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Step 3: Apply PCA\n",
    "pca = PCA(n_components=0.90)  # Retain 90% of the variance\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Step 4: Determine the number of components\n",
    "n_components = pca.n_components_\n",
    "\n",
    "# Step 5: Transform the data\n",
    "reduced_data = pca.transform(standardized_data)\n",
    "\n",
    "# Step 6: Use reduced data for model training\n",
    "# Continue with model training using 'reduced_data' as features\n",
    "```\n",
    "\n",
    "In this example:\n",
    "- We standardize the data using `StandardScaler`.\n",
    "- We apply PCA with the goal of retaining 90% of the variance.\n",
    "- We transform the standardized data using the selected principal components.\n",
    "- The reduced-dimensional dataset (`reduced_data`) can then be used for training your stock price prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q7.For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values: [ 1  5 10 15 20]\n",
      "Min-Max scaled values: [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Find the minimum and maximum values\n",
    "min_val = np.min(data)\n",
    "max_val = np.max(data)\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = (data - min_val) / (max_val - min_val)\n",
    "\n",
    "# Transform the scaled values to the range of -1 to 1\n",
    "scaled_data_new = 2 * scaled_data - 1\n",
    "\n",
    "print(\"Original values:\", data)\n",
    "print(\"Min-Max scaled values:\", scaled_data_new)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q8.For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of principal components to retain: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Define the dataset containing features: [height, weight, age, gender, blood pressure]\n",
    "data = np.array([\n",
    "    [160, 60, 30, 1, 120],\n",
    "    [170, 65, 35, 0, 130],\n",
    "    [180, 70, 40, 1, 140],\n",
    "    [165, 55, 25, 0, 110],\n",
    "    [175, 75, 45, 1, 150]\n",
    "])\n",
    "\n",
    "# Step 1: Standardize the data\n",
    "scaler = StandardScaler()\n",
    "standardized_data = scaler.fit_transform(data)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(standardized_data)\n",
    "\n",
    "# Step 3: Determine the number of components to retain based on explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of principal components to retain (e.g., retain enough components to explain 90% of the variance)\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.9) + 1\n",
    "\n",
    "print(\"Number of principal components to retain:\", n_components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ft",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
