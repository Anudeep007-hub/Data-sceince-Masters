{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Q1. A company conducted a survey of its employees and found that 70% of the employees use the company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the probability that an employee is a smoker given that he/she uses the health insurance plan?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the probability that an employee is a smoker given that they use the company's health insurance plan, we can use the information provided and apply Bayes' theorem.\n",
    "\n",
    "### Given Information\n",
    "1. **70% of the employees use the health insurance plan**: \\( P(U) = 0.70 \\)\n",
    "2. **40% of the employees who use the plan are smokers**: \\( P(S|U) = 0.40 \\)\n",
    "\n",
    "Here:\n",
    "- \\( U \\) represents the event that an employee uses the health insurance plan.\n",
    "- \\( S \\) represents the event that an employee is a smoker.\n",
    "\n",
    "We are asked to find \\( P(S|U) \\), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "### Solution\n",
    "The problem directly provides \\( P(S|U) \\), which is the probability of being a smoker given that the employee uses the health insurance plan. According to the given data:\n",
    "\n",
    "\\[ P(S|U) = 0.40 \\]\n",
    "\n",
    "So, the probability that an employee is a smoker given that they use the health insurance plan is **40%**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes and Multinomial Naive Bayes are both variants of the Naive Bayes classifier used for different types of data and applications. Here’s a detailed comparison of the two:\n",
    "\n",
    "### 1. **Type of Data**\n",
    "\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - **Features**: Binary or boolean features, indicating the presence or absence of a feature.\n",
    "  - **Data Representation**: Typically used when the features are represented as 0 or 1, meaning whether a feature is present or not (e.g., the presence or absence of words in text).\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - **Features**: Count data, representing the frequency of occurrences of features.\n",
    "  - **Data Representation**: Suitable for features that are counts or frequencies, such as word counts in text documents.\n",
    "\n",
    "### 2. **Underlying Assumptions**\n",
    "\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - Assumes that each feature is binary and follows a Bernoulli distribution.\n",
    "  - The model calculates the probability of a feature being present (1) or absent (0).\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - Assumes that the features are multinomially distributed, which means they represent counts or frequencies.\n",
    "  - The model calculates the probability of each feature occurring given a class, based on feature counts.\n",
    "\n",
    "### 3. **Usage**\n",
    "\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - Often used in text classification problems where the presence or absence of certain words or features is of interest.\n",
    "  - Examples include document classification based on whether certain keywords are present.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - Commonly used in text classification problems where the frequency of words or features is important.\n",
    "  - Examples include document classification based on word counts or term frequency-inverse document frequency (TF-IDF) values.\n",
    "\n",
    "### 4. **Mathematical Formulation**\n",
    "\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - The probability of a feature given a class is modeled using Bernoulli distribution:\n",
    "    \\[ P(x_i | y) = p_i^{x_i} \\cdot (1 - p_i)^{1 - x_i} \\]\n",
    "    where \\( x_i \\) is the binary feature (0 or 1), \\( p_i \\) is the probability of feature \\( i \\) being present given class \\( y \\), and \\( y \\) is the class.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - The probability of a feature given a class is modeled using Multinomial distribution:\n",
    "    \\[ P(x_i | y) = \\frac{(n_i + \\alpha - 1)!}{n_i! \\cdot (\\alpha - 1)!} \\cdot \\frac{p_i^{n_i}}{(1 - p_i)^{\\alpha - 1}} \\]\n",
    "    where \\( n_i \\) is the count of feature \\( i \\), \\( p_i \\) is the probability of feature \\( i \\) given class \\( y \\), and \\( \\alpha \\) is a smoothing parameter.\n",
    "\n",
    "### 5. **Example Applications**\n",
    "\n",
    "- **Bernoulli Naive Bayes**:\n",
    "  - Classifying emails as spam or not spam based on the presence or absence of specific words or phrases.\n",
    "  - Classifying text where binary features (e.g., presence of certain keywords) are used.\n",
    "\n",
    "- **Multinomial Naive Bayes**:\n",
    "  - Classifying text documents based on the frequency of words (e.g., how often each word appears in the document).\n",
    "  - Document classification tasks where word counts are significant.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Bernoulli Naive Bayes** is used when the features are binary and represent the presence or absence of something.\n",
    "- **Multinomial Naive Bayes** is used when the features are counts or frequencies of occurrences.\n",
    "\n",
    "Choosing between the two depends on the nature of your features and the type of data you have. If you’re dealing with binary features, Bernoulli Naive Bayes might be appropriate. If your features are counts or represent frequencies, Multinomial Naive Bayes is likely more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3. How does Bernoulli Naive Bayes handle missing values?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bernoulli Naive Bayes, like many machine learning algorithms, does not handle missing values directly. If you have missing values in your dataset, you need to preprocess your data before applying the Bernoulli Naive Bayes algorithm. Here are some common approaches to handling missing values:\n",
    "\n",
    "### 1. **Remove Missing Values**\n",
    "\n",
    "- **Description**: Simply remove rows with missing values from your dataset.\n",
    "- **When to Use**: If the proportion of missing data is small and removing these rows won’t significantly affect the dataset.\n",
    "- **Pros**: Simple to implement.\n",
    "- **Cons**: Can lead to loss of valuable data and may not be feasible if many rows contain missing values.\n",
    "\n",
    "### 2. **Impute Missing Values**\n",
    "\n",
    "- **Description**: Fill in missing values with a specific value or estimate. Common imputation methods include:\n",
    "  - **Mode Imputation**: Replace missing values with the most frequent value (mode) for the feature.\n",
    "  - **Zero Imputation**: Replace missing values with zero, assuming the feature’s absence is equivalent to zero (if this makes sense for the data).\n",
    "  - **Other Imputation Techniques**: Use advanced techniques like k-nearest neighbors (KNN) imputation, mean imputation, or regression imputation.\n",
    "\n",
    "- **When to Use**: When you want to retain all data but need to fill in missing values to make the dataset usable.\n",
    "- **Pros**: Allows you to use all data without removing rows.\n",
    "- **Cons**: Imputation can introduce biases or inaccuracies, especially if not done carefully.\n",
    "\n",
    "### 3. **Use a Binary Indicator for Missing Values**\n",
    "\n",
    "- **Description**: Create an additional binary feature indicating whether the original feature value was missing. Use this indicator feature along with the imputed or replaced values.\n",
    "- **When to Use**: When you want to preserve information about missingness as a separate feature.\n",
    "- **Pros**: Provides information about missing data which might be useful for some models.\n",
    "- **Cons**: Adds additional complexity to the model.\n",
    "\n",
    "### Example of Imputation for Bernoulli Naive Bayes\n",
    "\n",
    "If you’re using Bernoulli Naive Bayes and have missing values in your binary features, you might use the following imputation approach:\n",
    "\n",
    "1. **Impute Missing Values**: For each feature, replace missing values with zero (assuming missing means the feature is absent).\n",
    "\n",
    "    ```python\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    \n",
    "    # Create an imputer to replace missing values with 0\n",
    "    imputer = SimpleImputer(strategy='constant', fill_value=0)\n",
    "    \n",
    "    # Fit and transform the data\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    ```\n",
    "\n",
    "2. **Handle Missingness as a Separate Feature**: Create an indicator for missing values.\n",
    "\n",
    "    ```python\n",
    "    # Create a missing value indicator\n",
    "    missing_indicator = X.isna().astype(int)\n",
    "    \n",
    "    # Impute missing values\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    \n",
    "    # Combine original features with missing value indicators\n",
    "    X_combined = np.hstack([X_imputed, missing_indicator])\n",
    "    ```\n",
    "\n",
    "### Summary\n",
    "\n",
    "Bernoulli Naive Bayes does not inherently handle missing values. To use it effectively, you need to preprocess your data by either removing rows with missing values, imputing missing values, or using missing value indicators. The choice of method depends on the amount of missing data and the specific context of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4. Can Gaussian Naive Bayes be used for multi-class classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, Gaussian Naive Bayes can be used for multi-class classification. It is naturally suited for classification problems where the goal is to assign data points to one of multiple classes. \n",
    "\n",
    "### How Gaussian Naive Bayes Works for Multi-Class Classification\n",
    "\n",
    "Gaussian Naive Bayes assumes that the features of the data are normally distributed. For each class, the algorithm models the probability distribution of each feature using a Gaussian distribution (normal distribution). Here's how it handles multi-class classification:\n",
    "\n",
    "1. **Feature Distribution**: For each class \\( C_i \\), the features are assumed to be normally distributed. The mean (\\(\\mu_i\\)) and variance (\\(\\sigma_i^2\\)) of each feature are calculated for each class.\n",
    "\n",
    "2. **Likelihood Calculation**: Given a data point, the algorithm calculates the likelihood of the feature values under each class using the Gaussian probability density function.\n",
    "\n",
    "3. **Posterior Probability**: Bayes' theorem is used to calculate the posterior probability of each class given the feature values. This is done by combining the likelihood of the features given each class with the prior probability of each class.\n",
    "\n",
    "4. **Classification**: The class with the highest posterior probability is chosen as the predicted class for the data point.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "For a multi-class classification problem with \\( k \\) classes, the posterior probability \\( P(C_i | x) \\) for a class \\( C_i \\) given feature vector \\( x \\) is computed as:\n",
    "\n",
    "\\[ P(C_i | x) = \\frac{P(C_i) \\cdot \\prod_{j=1}^n P(x_j | C_i)}{P(x)} \\]\n",
    "\n",
    "where:\n",
    "- \\( P(C_i) \\) is the prior probability of class \\( C_i \\).\n",
    "- \\( P(x_j | C_i) \\) is the probability of feature \\( x_j \\) given class \\( C_i \\), modeled using a Gaussian distribution.\n",
    "- \\( P(x) \\) is the marginal likelihood of the feature vector \\( x \\), which is the same for all classes and can be used for normalization.\n",
    "\n",
    "For each feature \\( x_j \\) in class \\( C_i \\), the Gaussian distribution is:\n",
    "\n",
    "\\[ P(x_j | C_i) = \\frac{1}{\\sqrt{2 \\pi \\sigma_{j,i}^2}} \\exp \\left( - \\frac{(x_j - \\mu_{j,i})^2}{2 \\sigma_{j,i}^2} \\right) \\]\n",
    "\n",
    "where:\n",
    "- \\( \\mu_{j,i} \\) is the mean of feature \\( x_j \\) in class \\( C_i \\).\n",
    "- \\( \\sigma_{j,i}^2 \\) is the variance of feature \\( x_j \\) in class \\( C_i \\).\n",
    "\n",
    "### Example\n",
    "\n",
    "Consider a classification problem where you want to classify flowers into one of three species based on measurements like petal length and width. Gaussian Naive Bayes will:\n",
    "\n",
    "1. Compute the mean and variance of petal length and width for each flower species.\n",
    "2. For a new flower, compute the likelihood of its petal measurements for each species using the Gaussian distribution.\n",
    "3. Apply Bayes' theorem to determine the most probable species given the measurements.\n",
    "\n",
    "### Summary\n",
    "\n",
    "Gaussian Naive Bayes can handle multi-class classification effectively by modeling each class with a Gaussian distribution for each feature. It calculates the posterior probabilities for each class and assigns the data point to the class with the highest posterior probability. This makes it a versatile algorithm for problems with more than two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
